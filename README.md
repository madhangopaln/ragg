# ragg


## run (step-by-step)
1. create the project files
2. Copy .env.ex to .env and set HF_API_KEY.
3. Create Python venv and install deps:

    python -m venv .venv
    venv\Scripts\activate    
    pip install -r requirements.txt
    pip install streamlit

ğŸ“‚ Project Structure
ragg/
â”œâ”€â”€ app/                    
â”‚    â”œâ”€â”€ config.py            # configuration
â”‚    â”œâ”€â”€ db.py               # database config
â”‚    â”œâ”€â”€ embeddings.py       # LangChain Embeddings wrapper
â”‚    â”œâ”€â”€ hfclient.py         # HF Inference API calls
â”‚    â”œâ”€â”€ main.py             # FastAPI app
â”‚    â”œâ”€â”€ models.py           # schema
â”‚    â”œâ”€â”€ rag.py              # ingestion + storage + vector store
â”‚    â”œâ”€â”€ schemas.py          # FastAPI server
â”œâ”€â”€ tests/                  
â”‚    â”œâ”€â”€ test_api.py         # testing
â”œâ”€â”€  streamlit_app.py        # Streamlit frontend       
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€  requirements            # Python dependencies
â”œâ”€â”€  .env                    # Environment variables ( API keys)




ğŸ“Œ AI Chatbot with Document Upload (FastAPI + Streamlit + LangChain + Hugging Face)

This project implements a simple AI chatbot that allows users to upload multiple PDF (or text/docx) documents, extracts text, generates embeddings, stores metadata in a SQL database, and answers user queries using a RAG (Retrieval-Augmented Generation) pipeline.
```bash

ğŸ“‚ Project Structure
ai-chatbot/
â”‚â”€â”€ README.md                # Project documentation (this file)
â”‚â”€â”€ requirements.txt          # Python dependencies
â”‚â”€â”€ .env                      # Environment variables (HF token, DB URL, etc.)
â”‚â”€â”€ docker-compose.yml        # For local deployment
â”‚â”€â”€ Dockerfile                # Container setup
â”‚
â”œâ”€â”€ backend/                  # FastAPI backend
â”‚   â”œâ”€â”€ main.py               # FastAPI entry point
â”‚   â”œâ”€â”€ database.py           # SQLAlchemy DB connection
â”‚   â”œâ”€â”€ models.py             # SQLAlchemy models (File, Chunk)
â”‚   â”œâ”€â”€ rag.py                # RAG pipeline (chunking, embeddings, retrieval, generation)
â”‚   â”œâ”€â”€ hf_client.py          # Hugging Face API client (embedding + generation calls)
â”‚   â”œâ”€â”€ tests/                # Unit & integration tests
â”‚   â”‚   â”œâ”€â”€ test_api.py       # Tests for FastAPI endpoints
â”‚   â”‚   â””â”€â”€ test_rag.py       # Tests for RAG pipeline
â”‚   â””â”€â”€ uploads/              # Uploaded documents (saved on server)
â”‚
â”œâ”€â”€ frontend/                 # Streamlit frontend
â”‚   â””â”€â”€ app.py                # Streamlit UI (file upload + chat interface)
â”‚
â””â”€â”€ migrations/ (optional)    # Alembic migrations if using for DB schema versioning
```

ğŸš€ How It Works

Upload PDFs â†’ Extract text with PyPDF2/docx/txt parsing.

Chunking â†’ Split documents into overlapping text chunks.

Embeddings â†’ Create embeddings using Hugging Face models (sentence-transformers).

Database â†’ Save file metadata + embeddings with SQLAlchemy (SQLite by default).

Query â†’ Retrieve most relevant chunks with cosine similarity.

RAG pipeline â†’ Generate answers with Hugging Face text generation model.

Frontend â†’ Streamlit app provides an easy-to-use UI.

ğŸ› ï¸ Setup in VS Code

Clone the repo:

git clone https://github.com/yourusername/ai-chatbot.git
cd ai-chatbot


Create a virtual environment:

python -m venv venv
source venv/bin/activate      # Mac/Linux
venv\Scripts\activate         # Windows


Install dependencies:

pip install -r requirements.txt


Configure .env file:

HF_TOKEN=your_huggingface_api_key
DATABASE_URL=sqlite:///./backend/files.db


Run FastAPI backend:

uvicorn backend.main:app --reload


Run Streamlit frontend:

streamlit run frontend/app.py


Open:

Backend API docs â†’ http://127.0.0.1:8000/docs

Streamlit UI â†’ http://localhost:8501

ğŸ§ª Testing

Run all unit tests with:

pytest backend/tests/ -v

ğŸ³ Docker Deployment
docker-compose up --build


This will start both FastAPI and Streamlit services in containers.


